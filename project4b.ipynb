{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1488c85",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Project 4b: Goals and Deliverables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09d8c13",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "The goals of this assignment are:\n",
    "* To work with the object oriented version of our corpus code.\n",
    "* To modify a web app that we can use to analyze text data.\n",
    "* To explore document- and corpus-level analyses using transformer models: summarization, key phrase extraction, and sentiment and topic analysis.\n",
    "\n",
    "\n",
    "Here are the steps you should do to successfully complete this project:\n",
    "1. From moodle, download the files for this project. Upload them into the codespace for project 4a.\n",
    "2. Complete the notebook and commit it to Github. Make sure to answer all questions, and to commit the notebook in a \"run\" state!\n",
    "3. Modify `spacy_on_corpus.py` following the instructions in this notebook.\n",
    "4. Edit the README.md file. Provide your name, your class year, links to/descriptions of any extensions and a list of resources. \n",
    "5. Commit your code often. We will take the last commit before the deadline as your submission of the project.\n",
    "\n",
    "Possible extensions (from least points to most points):\n",
    "\n",
    "Possible extensions (from least points to most points):\n",
    "\n",
    "* Modify the token, entity, and noun chunk get count methods so they count only lower cased tokens, entities and noun chunks.\n",
    "* Modify the [styling](https://anvil.works/learn/tutorials/using-material-3) of the web app. \n",
    "* To the screen `Build Corpus` in the web app, add the ability for the user to choose the language of their input documents.\n",
    "* To the screen `Analyze Document` in the web app, add the ability for the user to choose a value for `top_k` and to choose which token and entity tags to exclude.\n",
    "* Plot more than one analysis at a time in `Analyze Corpus` (see [this page](https://anvil.works/docs/client/components/plots)).\n",
    "* Add the ability for a user to enter jsonl in the input text area on the `Build Corpus` screen.\n",
    "* If you added paragraphs to project 3c, port that over to project 4a.\n",
    "* Add some metadata analysis and visualization on a fourth screen.\n",
    "* Your other ideas are welcome! If you'd like to discuss one with Dr Stent, feel free."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Up\n",
    "\n",
    "1. Download this notebook, the `requirements.txt` file and the file `creator.jsonl`.\n",
    "2. Upload all three into your project 4a codespace.\n",
    "3. Run % `pip install -r requirements.txt`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Sure We Can Work With .py Files We Are Editing\n",
    "\n",
    "Run the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Automatically reload your external source code\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get a Corpus\n",
    "\n",
    "In the code cell below, build a corpus using `creator.jsonl`. This will be our test corpus for this project. If you can get `files.jsonl.zip` to load you can use it at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't load creator.jsonl due to error text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy_on_corpus import corpus\n",
    "my_corpus = corpus()\n",
    "corpus.build_corpus('creator.jsonl', my_corpus= my_corpus)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ways We Can Add More NLP\n",
    "\n",
    "There are many python packages that do NLP. Today we will look at three ways to add more NLP to our `corpus` class and our web app:\n",
    "\n",
    "1. Augment spaCy\n",
    "2. Use huggingface transformers\n",
    "3. Use something else (that probably uses huggingface transformers!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternative 1: Augment spaCy\n",
    "\n",
    "spaCy has extra plugins (available from the [spaCy universe](https://spacy.io/universe/) ). These plugins allow you to extend spaCy. We will play with two."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key phrases\n",
    "\n",
    "Let's get some keyphrases!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['beautiful in the fall', 'quiet roads']\n"
     ]
    }
   ],
   "source": [
    "import pyate\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')          \n",
    "nlp.add_pipe('combo_basic')\n",
    "\n",
    "doc = nlp('Maine is beautiful in the fall. The leaves turn orange and green and drop from the trees. The quiet roads summon all travelers.')\n",
    "print(list(doc._.combo_basic.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add key phrases to the things we can do in our `corpus` class\n",
    "\n",
    "Modify `spacy_on_corpus.py` as follows:\n",
    "1. import `pyate`\n",
    "2. after you make the spacy engine, add this line: `nlp.add_pipe(\"combo_basic\")`\n",
    "3. implement instance method `get_keyphrase_counts` which behaves similarly to `get_token_counts` except the document attribute to retrieve is `_.combo_basic`\n",
    "4. implement instance method `get_keyphrase_statistics`\n",
    "\n",
    "Make sure to add docstrings.\n",
    "\n",
    "Feel free to test in the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(my_corpus.get_token_counts())\n",
    "my_corpus.get_keyphrase_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add keyphrases to our web app's server\n",
    "\n",
    "In the notebook for project4a, define a function `get_corpus_keyphrases_statistics` that returns a list of sentiment types and their frequencies (a counter of positive sentiment, negative sentiment and neutral sentiment documents in the corpus).\n",
    "\n",
    "\n",
    "Make sure to add docstrings.\n",
    "\n",
    "Feel free to test in the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add keyphrases to our web app's client\n",
    "\n",
    "1. Add a button for key phrases to the `Analyze Corpus` form\n",
    "2. Add a function in the code for that form that calls `get_keyphrase_statistics`\n",
    "3. Add a plot of the sentiment counts to the web app\n",
    "4. If the user chooses key phrases and either count or cloud, print an error message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "1. *What is `_.combo_basic'? Is it an instance attribute, instance method, class attribute or class method of class `doc`?*\n",
    "Instance attribute because _.combo_basic is an attribute relating to an instance of a doc class. We run the .keys() method on it.\n",
    "2. *Why do we not implement `plot_keyphrase_counts` or `plot_keyphrase_cloud`?*\n",
    "Because most of the keyphrase counts are unique it wouldn't give us much information. \n",
    "3. *Rebuild your corpus now that you have added functionality. Look at the keyphrases. Do you agree that these capture the essence of this corpus?*\n",
    "Yes, they provide lots of the important core data about the corpus. If looking for quick synthesis keywords can give core meaning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternative 2: Use huggingface transformers\n",
    "\n",
    "[Huggingface](https://www.crunchbase.com/organization/hugging-face/) is a Brooklyn-based company that was founded just about the time the first transformer models for NLP became famous. Its business model is open source.\n",
    "\n",
    "The huggingface staff:\n",
    "\n",
    "* host transformer-related data, code, models, data sheets, model cards and applications\n",
    "* help people more easily use transformers (for NLP, computer vision and other AI applications)\n",
    "* help people more easily *fine tune* transformers (we will do that!)\n",
    "* consult with companies on how to operationalize and scale their use of transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `transformers`\n",
    "\n",
    "Because of the huggingface `transformers` package, we can easily use transformers ourselves!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis\n",
    "\n",
    "Let's make a transformers `pipeline` for sentiment analysis. Sentiment analysis is a NLP task that estimates the *polarity* (and sometimes the *strength*) of the sentiment communicated by a text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9997126460075378}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "print(classifier('Maine is beautiful in the fall. The leaves turn orange and green and drop from the trees. The quiet roads summon all travelers.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's get key phrases from each document in our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[]\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "# get a list of the document ids in the corpus\n",
    "keys = list(my_corpus.keys())\n",
    "# get the text from each entry in the corpus\n",
    "text = my_corpus.get_documents()\n",
    "# there are two ways to get the text from each entry in the corpus!\n",
    "\n",
    "# print the texts\n",
    "print(*text, sep = \"\\n\")\n",
    "# do sentiment analysis\n",
    "results = []\n",
    "for doc in text:\n",
    "    results.extend(classifier(str(doc)))\n",
    "# print the results\n",
    "print(results)\n",
    "# add the sentiment label and score into the metadata for each entry in the corpus\n",
    "for i, result in enumerate(results):\n",
    "    my_corpus[keys[i]]['metadata']['sentiment_label'] = result['label']\n",
    "    my_corpus[keys[i]]['metadata']['sentiment_score'] = result['score']\n",
    "print(my_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, we just used transformers, the most advanced NLP model type known today! \n",
    "\n",
    "A huggingface `pipeline` pulls together a tokenizer, one or more models, and some post-processing. It can operate over a single text or a list of texts.\n",
    "\n",
    "[There are NLP pipelines](https://huggingface.co/docs/transformers/main_classes/pipelines) for:\n",
    "\n",
    "* named entity recognition\n",
    "* sentiment analysis\n",
    "* summarization\n",
    "* question answering\n",
    "* text classification\n",
    "* translation\n",
    "\n",
    "There are also computer vision and speech pipelines.\n",
    "\n",
    "You can change the sentiment model. Copy the code from above into a new code cell. Investigate the sentiment models available - try at least one more. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to t5-base and revision 686f1db (https://huggingface.co/t5-base).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py:160: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier2 = pipeline(\"translation_en_to_fr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[]\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "# get a list of the document ids in the corpus\n",
    "keys = list(my_corpus.keys())\n",
    "# get the text from each entry in the corpus\n",
    "text = my_corpus.get_documents()\n",
    "# there are two ways to get the text from each entry in the corpus!\n",
    "\n",
    "# print the texts\n",
    "print(*text, sep = \"\\n\")\n",
    "# do sentiment analysis\n",
    "results = []\n",
    "for doc in text:\n",
    "    results.extend(classifier2(str(doc)))\n",
    "# print the results\n",
    "print(results)\n",
    "# add the sentiment label and score into the metadata for each entry in the corpus\n",
    "for i, result in enumerate(results):\n",
    "    my_corpus[keys[i]]['metadata']['sentiment_label'] = result['translation_text']\n",
    "print(my_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A little abstraction\n",
    "\n",
    "Before we proceed, let's make a function to get the texts of all the documents out of the corpus.\n",
    "\n",
    "In `spacy_on_corpus.py`:\n",
    "* add an instance method to class `corpus` called `get_document_texts`. It should return a list of pairs (id, text).\n",
    "* add an instance method to class `corpus` called `update_document_metadata`. It should take a document id and a dictionary of metadata key:value pairs. It should add each key:value pair to the document in the corpus at the id, and if there is no such document id in the corpus it should print an error message.\n",
    "\n",
    "\n",
    "\n",
    "Now copy the code from above into a new code cell. Modify it to use `get_document_texts` and `update_document_metadata`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[]\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "texts = my_corpus.get_document_texts()\n",
    "# there are two ways to get the text from each entry in the corpus\n",
    "# print the texts\n",
    "print(*texts, sep = \"\\n\")\n",
    "# do sentiment analysis\n",
    "results = []\n",
    "for doc in texts:\n",
    "    results.extend(classifier2(str(doc[1])))\n",
    "# print the results\n",
    "print(results)\n",
    "# add the sentiment label and score into the metadata for each entry in the corpus\n",
    "for i, result in enumerate(results):\n",
    "    my_corpus.update_document_metadata(texts[i][0], result)\n",
    "print(my_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add sentiment analysis to our `corpus` class\n",
    "\n",
    "In the `corpus` class, add some code to run sentiment analysis to the `add_document` method. \n",
    "\n",
    "In `get_basic_statistics`, print out the number of documents, number of sentences and number of positive, neutral and negative documents.\n",
    "\n",
    "Use the code cell below to test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1': {'doc': It's a shame that the weak writing undermines The Creator so much, as there are some intriguing concepts that could have been compelling if executed better. For the most part, it's a mishmash of other movies with not much to say on its own., 'metadata': {'id': '1', 'author': 'Michelle Kisner', 'fullText': \"It's a shame that the weak writing undermines The Creator so much, as there are some intriguing concepts that could have been compelling if executed better. For the most part, it's a mishmash of other movies with not much to say on its own.\"}, 'sentiment-analysis': [{'label': 'NEGATIVE', 'score': 0.9997203946113586}]}, '2': {'doc': Although 'New Asia' is America's enemy, we are encouraged to transfer our sympathies in that direction. Yet the abiding vision of Asian life is a mass of touristic clichés seen through western eyes., 'metadata': {'id': '2', 'author': 'John McDonald', 'fullText': \"Although 'New Asia' is America's enemy, we are encouraged to transfer our sympathies in that direction. Yet the abiding vision of Asian life is a mass of touristic clichés seen through western eyes.\"}, 'sentiment-analysis': [{'label': 'NEGATIVE', 'score': 0.9748802781105042}]}, '3': {'doc': The Creator has astonishing visuals, but that's where its charm ends. While the performances are strong, thrilling elements of the film swap actual excitement for a more traditional science fiction film that pays tribute to its influences and little else., 'metadata': {'id': '3', 'author': 'Chris Sawin', 'fullText': \"The Creator has astonishing visuals, but that's where its charm ends. While the performances are strong, thrilling elements of the film swap actual excitement for a more traditional science fiction film that pays tribute to its influences and little else.\"}, 'sentiment-analysis': [{'label': 'POSITIVE', 'score': 0.9934230446815491}]}, '4': {'doc': The Creator is incredibly immersive from a visual and world building perspective, however it leaves a lot to be desired with its writing., 'metadata': {'id': '4', 'author': 'JD Duran', 'fullText': 'The Creator is incredibly immersive from a visual and world building perspective, however it leaves a lot to be desired with its writing.'}, 'sentiment-analysis': [{'label': 'NEGATIVE', 'score': 0.9772546291351318}]}, '5': {'doc': The Creator is a gorgeous feature in all the ways that matter and it's certainly a sci-fi epic that shouldn't be missed in a year that's severely lacking the charm of an old-fashioned blockbuster!, 'metadata': {'id': '5', 'author': 'Connor Petrey', 'fullText': \"The Creator is a gorgeous feature in all the ways that matter and it's certainly a sci-fi epic that shouldn't be missed in a year that's severely lacking the charm of an old-fashioned blockbuster!\"}, 'sentiment-analysis': [{'label': 'POSITIVE', 'score': 0.9936063289642334}]}, '6': {'doc': Sincere in its execution and with at times a mystifying beauty, as well as some serious thrills, Edwards has yet again rewarded audiences with his leap of faith., 'metadata': {'id': '6', 'author': 'James Hanton', 'fullText': 'Sincere in its execution and with at times a mystifying beauty, as well as some serious thrills, Edwards has yet again rewarded audiences with his leap of faith.'}, 'sentiment-analysis': [{'label': 'POSITIVE', 'score': 0.9998550415039062}]}, '7': {'doc': The Creator is a thought-provoking, visually memorizing, sci-fi journey that'll tantalize the imagination. Director Gareth Edwards put together an intellectually stimulating film that really challenges audiences to wrestle with profound questions., 'metadata': {'id': '7', 'author': 'Emmanuel Noisette', 'fullText': \"The Creator is a thought-provoking, visually memorizing, sci-fi journey that'll tantalize the imagination. Director Gareth Edwards put together an intellectually stimulating film that really challenges audiences to wrestle with profound questions.\"}, 'sentiment-analysis': [{'label': 'POSITIVE', 'score': 0.9998666048049927}]}, '8': {'doc': Writer-director Gareth Edwards and his collaborator, Chris Weitz, have crafted a script which tries so hard to cover all the ethical dilemmas posed by the rise of artificial intelligence that I'm not sure what it's meant to be saying., 'metadata': {'id': '8', 'author': 'Sandra Hall', 'fullText': \"Writer-director Gareth Edwards and his collaborator, Chris Weitz, have crafted a script which tries so hard to cover all the ethical dilemmas posed by the rise of artificial intelligence that I'm not sure what it's meant to be saying.\"}, 'sentiment-analysis': [{'label': 'NEGATIVE', 'score': 0.9866374731063843}]}, '9': {'doc': The Creator is a 2023 American science fiction action film produced and directed by Gareth Edwards, who co-wrote the screenplay with Chris Weitz. The film stars John David Washington, Gemma Chan, Ken Watanabe, Sturgill Simpson and Allison Janney. Set in 2070, 15 years after a nuclear detonation in Los Angeles and a war against artificial intelligence, an ex-special forces agent is recruited to hunt down and kill the 'Creator,' who has developed a mysterious weapon with the power to end the war., 'metadata': {'id': '9', 'author': 'Wikipedia', 'fullText': \"The Creator is a 2023 American science fiction action film produced and directed by Gareth Edwards, who co-wrote the screenplay with Chris Weitz. The film stars John David Washington, Gemma Chan, Ken Watanabe, Sturgill Simpson and Allison Janney. Set in 2070, 15 years after a nuclear detonation in Los Angeles and a war against artificial intelligence, an ex-special forces agent is recruited to hunt down and kill the 'Creator,' who has developed a mysterious weapon with the power to end the war.\"}, 'sentiment-analysis': [{'label': 'POSITIVE', 'score': 0.9977133274078369}]}, '10': {'doc': Development on the film began in November 2019, when Gareth Edwards signed on to direct and write an untitled science fiction project for New Regency to produce, along with Edwards' Rogue One: A Star Wars Story (2016) co-producer Kiri Hart serving as producer. A test shoot and location scouting was conducted that year, with Edwards using it as the opportunity to envision the look of the film. He described the process: 'I took a camera and a 1970s anamorphic lens, we went location-scouting in Vietnam, Cambodia, Japan, Indonesia, Thailand, and Nepal. Our whole plan was just to go to the greatest locations in the world, because the cost of a flight is way less than the cost of building a set. We were going to hopscotch around the world and shoot this film, then layer in the science-fiction on top afterwards. If our film is trying to achieve something visually, it's trying to feel real in terms of science-fiction.', 'metadata': {'id': '10', 'author': 'Wikipedia', 'fullText': \"Development on the film began in November 2019, when Gareth Edwards signed on to direct and write an untitled science fiction project for New Regency to produce, along with Edwards' Rogue One: A Star Wars Story (2016) co-producer Kiri Hart serving as producer. A test shoot and location scouting was conducted that year, with Edwards using it as the opportunity to envision the look of the film. He described the process: 'I took a camera and a 1970s anamorphic lens, we went location-scouting in Vietnam, Cambodia, Japan, Indonesia, Thailand, and Nepal. Our whole plan was just to go to the greatest locations in the world, because the cost of a flight is way less than the cost of building a set. We were going to hopscotch around the world and shoot this film, then layer in the science-fiction on top afterwards. If our film is trying to achieve something visually, it's trying to feel real in terms of science-fiction.'\"}, 'sentiment-analysis': [{'label': 'POSITIVE', 'score': 0.6624099016189575}]}}\n"
     ]
    }
   ],
   "source": [
    "from spacy_on_corpus import corpus\n",
    "my_corpus = corpus()\n",
    "my_corpus = corpus.build_corpus('creator.jsonl', my_corpus= my_corpus)\n",
    "print(my_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/workspaces/project-4a-the-web-app-benjamin-wintersteen/project4b.ipynb Cell 31\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://codespaces%2Blaughing-umbrella-r4gxjjpqqwj72xrv9/workspaces/project-4a-the-web-app-benjamin-wintersteen/project4b.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(my_corpus\u001b[39m.\u001b[39;49mget_basic_statistics())\n",
      "File \u001b[0;32m/workspaces/project-4a-the-web-app-benjamin-wintersteen/spacy_on_corpus.py:325\u001b[0m, in \u001b[0;36mcorpus.get_basic_statistics\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    323\u001b[0m text \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_entity_statistics()\n\u001b[1;32m    324\u001b[0m text \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_noun_chunk_statistics()\n\u001b[0;32m--> 325\u001b[0m text \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_sentiment_statistics()\n\u001b[1;32m    326\u001b[0m \u001b[39mreturn\u001b[39;00m text\n",
      "File \u001b[0;32m/workspaces/project-4a-the-web-app-benjamin-wintersteen/spacy_on_corpus.py:309\u001b[0m, in \u001b[0;36mcorpus.get_sentiment_statistics\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Prints summary statistics for noun chunks in the corpus. Model on get_token_statistics.\u001b[39;00m\n\u001b[1;32m    304\u001b[0m \u001b[39m\u001b[39;00m\n\u001b[1;32m    305\u001b[0m \u001b[39m:returns: the statistics report\u001b[39;00m\n\u001b[1;32m    306\u001b[0m \u001b[39m:rtype: str\u001b[39;00m\n\u001b[1;32m    307\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[39m# NEW FOR PROJECT 4a\u001b[39;00m\n\u001b[0;32m--> 309\u001b[0m sentiment_counts \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_sentiment_counts()\n\u001b[1;32m    310\u001b[0m text \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mPositive Documents: %i\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m \u001b[39msum\u001b[39m([x[\u001b[39m1\u001b[39m] \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m sentiment_counts \u001b[39mif\u001b[39;00m x[\u001b[39m0\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mPOSITIVE\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m    311\u001b[0m text \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNeutral Docuements: %i\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39msum\u001b[39m([x[\u001b[39m1\u001b[39m] \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m sentiment_counts \u001b[39mif\u001b[39;00m x[\u001b[39m0\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mNEUTRAL\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "File \u001b[0;32m/workspaces/project-4a-the-web-app-benjamin-wintersteen/spacy_on_corpus.py:241\u001b[0m, in \u001b[0;36mcorpus.get_sentiment_counts\u001b[0;34m(self, top_k)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[39m# For each doc in the corpus, add its chunks to the list of chunks (2 lines)\u001b[39;00m\n\u001b[1;32m    240\u001b[0m \u001b[39mfor\u001b[39;00m \u001b[39mid\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 241\u001b[0m     sentiments\u001b[39m.\u001b[39mextend(\u001b[39mid\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39msentiment-analysis\u001b[39;49m\u001b[39m'\u001b[39;49m][\u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m    242\u001b[0m \u001b[39m# Count the chunks using a counter object; return a list of pairs (item, frequency) (1 line)\u001b[39;00m\n\u001b[1;32m    243\u001b[0m \u001b[39mreturn\u001b[39;00m counter(sentiments, top_k \u001b[39m=\u001b[39m top_k)\u001b[39m.\u001b[39mget_counts()\n",
      "\u001b[0;31mTypeError\u001b[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "\n",
    "print(my_corpus.get_basic_statistics())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add sentiment analysis to our web app's server\n",
    "\n",
    "In the notebook for project4a:\n",
    "\n",
    "1. define a function `get_corpus_sentiment` that returns a list of sentiment types and their frequencies (a counter of positive sentiment, negative sentiment and neutral sentiment documents in the corpus).\n",
    "2. define a function `get_corpus_statistics` that calls `get_basic_statistics`.\n",
    "\n",
    "Use the code cell below to test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add sentiment analysis to our web app's client\n",
    "\n",
    "In your web app, add the ability to plot corpus sentiment:\n",
    "1. Add a button for sentiment to the `Analyze Corpus` form\n",
    "2. Add a function in the code for that form that calls `get_corpus_sentiment`\n",
    "3. Add a plot of the sentiment counts to the web app\n",
    "4. On the `Build Corpus` form, print the basic statistics after each action is performed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "4. *Why did we add `update_document_metadata` to our corpus class?*\n",
    "If we want to add new metadata to a document we can with this method\n",
    "5. *Run the sentiment analysis on this corpus. For which documents do you agree with the assigned sentiment, and for which do you disagree?*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarization\n",
    "\n",
    "Let's try a transformers single-document summarization pipeline.\n",
    "\n",
    "In the code cell below, summarize each document. Add each summary to the corresponding document's metadata in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "summarizer = pipeline(\"summarization\")\n",
    "\n",
    "# get the documents and their ids\n",
    "texts = my_corpus.get_document_texts()\n",
    "# summarize\n",
    "results = []\n",
    "for doc in texts:\n",
    "    results.extend(summarizer(str(texts[1])))\n",
    "# add the summaries to the documents' metadatas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe we think those summaries are too long or too short. Let's exert more control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the documents and their ids\n",
    "\n",
    "# summarize\n",
    "results = summarizer(..., min_length=5, max_length=20)\n",
    "\n",
    "# add the summaries to the documents' metadatas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is that better?\n",
    "\n",
    "What if we used a different model? In the code cell below, try a different summarizer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is that better?\n",
    "\n",
    "Notice that when you instantiate a pipeline, hugginface downloads a model. Any transformer model is pretty big. Some are a lot bigger than others. Downloading a model (and then loading it) takes time, which is why once we've made a pipeline it's good to keep it around if we are going to process a lot of documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add summarization to our `corpus` class\n",
    "\n",
    "In the `corpus` class, add some code to run single-document summarization to the `add_document` method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add summarization to our web app's server\n",
    "\n",
    "In the notebook for project4a, define a function `get_document_summary` that gets the summary for a single document id. It should return the text summary.\n",
    "\n",
    "Use the code cell below to test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add summarization our web app's client\n",
    "\n",
    "In your web app, add the ability to display a document summary:\n",
    "1. Add a button for summary to the `Analyze Document` form\n",
    "2. Add a function in the code for that form that calls `get_document_summary`\n",
    "3. Render the document summary in the web app"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "6. *Look at the generated summaries. Are they extractive or abstractive? In other words, is every word in a summary in the document it comes from?*\n",
    "7. *Play around with summary lengths. Is there a good length - either an absolutely ideal length, or a fraction of the length of the input document? Why or why not?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternative 3: Using transformers in Other Packages\n",
    "\n",
    "Transformers are used in many other applications and python packages. Here we will look at one, BERTopic, which does topic modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, get the full texts from the corpus\n",
    "# get a lot of copies of them since we have a small corpus and topic modeling needs more documents\n",
    "full_texts = [my_corpus[x]['doc'].text for x in my_corpus]*50\n",
    "\n",
    "print(len(full_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic modeling\n",
    "\n",
    "We set up BERTTopic with spaCy using the best practices from https://maartengr.github.io/BERTopic/getting_started/best_practices/best_practices.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-calculate embeddings; consider any embedding model\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "embeddings = embedding_model.encode(full_texts, show_progress_bar=True)\n",
    "\n",
    "# Prevent stochastic behavior\n",
    "from umap import UMAP\n",
    "\n",
    "# choose a number of neighbors that's reasonable for your data set\n",
    "umap_model = UMAP(n_neighbors=5, n_components=5, min_dist=0.0, metric='cosine', random_state=42)\n",
    "\n",
    "# Set minimum cluster size\n",
    "from hdbscan import HDBSCAN\n",
    "\n",
    "# choose a minimum cluster size that's reasonable for your data set\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=5, metric='euclidean', cluster_selection_method='eom', prediction_data=True)\n",
    "\n",
    "# I find this dubious but okay; \"Improve\" default representation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer_model = CountVectorizer(stop_words=\"english\", min_df=2, ngram_range=(1, 2))\n",
    "\n",
    "# Use multiple representations\n",
    "from bertopic.representation import KeyBERTInspired, MaximalMarginalRelevance, PartOfSpeech\n",
    "\n",
    "# KeyBERT\n",
    "keybert_model = KeyBERTInspired()\n",
    "\n",
    "# Part-of-Speech\n",
    "pos_model = PartOfSpeech(\"en_core_web_md\")\n",
    "\n",
    "# MMR\n",
    "mmr_model = MaximalMarginalRelevance(diversity=0.3)\n",
    "\n",
    "# All representation models\n",
    "representation_model = {\n",
    "    \"KeyBERT\": keybert_model,\n",
    "    \"MMR\": mmr_model,\n",
    "    \"POS\": pos_model\n",
    "}\n",
    "\n",
    "# Make topic model using all of this setup\n",
    "from bertopic import BERTopic\n",
    "\n",
    "topic_model = BERTopic(\n",
    "\n",
    "  # Pipeline models\n",
    "  embedding_model=embedding_model,\n",
    "  umap_model=umap_model,\n",
    "  hdbscan_model=hdbscan_model,\n",
    "  vectorizer_model=vectorizer_model,\n",
    "  representation_model=representation_model,\n",
    "\n",
    "  # Hyperparameters\n",
    "  top_n_words=10,\n",
    "  verbose=True,\n",
    "  nr_topics=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That bit takes awhile. Now we have a topic model, but what does it look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the topics\n",
    "topics, probabilities = topic_model.fit_transform(full_texts, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now at this point you will iterate through one or both of the steps below til you are happy(ish).\n",
    "\n",
    "Look at the topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a visualization of the topic model for this group\n",
    "topic_model.visualize_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or maybe visualizing the documents will help more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce dimensionality of embeddings, this step is optional but much faster to perform iteratively:\n",
    "reduced_embeddings = UMAP(n_neighbors=10, n_components=2, min_dist=0.0, metric='cosine').fit_transform(embeddings)\n",
    "topic_model.visualize_documents(full_texts, reduced_embeddings=reduced_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe you look at the visualization and you know the number of topics you want to end up with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.reduce_topics(full_texts, nr_topics=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or maybe you want to merge some topics, like topics 1 and 2 and topics 5 and 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_to_merge = [[0, 1]]\n",
    "topic_model.merge_topics(full_texts, topics_to_merge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add topic modeling to our `corpus` class\n",
    "\n",
    "In the `corpus` class, add an instance method `build_topic_model` that runs topic modeling. \n",
    "\n",
    "Use the code cell below to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add topic modeling to our web app's server\n",
    "\n",
    "In the notebook for project4a, define a function `get_topic_model_topics_plot` that returns the topic model plot.\n",
    "\n",
    "In the notebook for project4a, define a function `get_topic_model_documents_plot` that returns the topic model document plot.\n",
    "\n",
    "Use the code cell below to test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Add topic modeling to our web app's client\n",
    "\n",
    "In your web app, add the ability to display a document summary:\n",
    "1. Add a new form for topic modeling, called `Analyze Topics`\n",
    "2. Add it to the top right hand menu\n",
    "3. Add a function in the code for that form that calls `get_topic_model_topics_plot` and `get_topic_model_documents_plot`\n",
    "4. Render the two plots in the web app, side by side or one above the other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "8. *Can a document have two or more topics? Why or why not?*\n",
    "9. *Which do you think is more easy to understand as a 'capture' of a corpus - key phrases or topics? Why?*\n",
    "10. *Which flow control statement type do we use more often in the `corpus` class: for, while or if?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Huggingface vs spaCy\n",
    "\n",
    "Huggingface and spaCy are different companies. Each company releases open source software.\n",
    "\n",
    "The huggingface software is the python package `transformers`.\n",
    "\n",
    "The spaCy software is the python package `spaCy`.\n",
    "\n",
    "Both softwares use models. spaCy has a whole set of models (the ones ending in `-trf`) that use huggingface transformers!\n",
    "\n",
    "spaCy can do some NLP tasks that huggingface can't do. \n",
    "\n",
    "The spaCy models are highly tuned and optimized for processing text using NLP. The huggingface models (e.g. for NER) are contributed by the community. \n",
    "\n",
    "The huggingface models focus more on NLP *applications* like summarization, sentiment analysis or translation.\n",
    "\n",
    "If you have a choice, I would use the spaCy models for text preprocessing. \n",
    "\n",
    "If you want to use a NLP application, huggingface is great."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
